--- 
layout: default
--- 


<h2 >Publications</h2>
<div>
<li><b>Self-Knowledge Guided Retrieval Augmentation for Large Language Models.</b><br/>
  &nbsp;&nbsp;&nbsp;&nbsp;<b>Yile Wang</b>, Peng Li, Maosong Sun and Yang Liu. <br/>
  &nbsp;&nbsp;&nbsp;&nbsp;In Findings of the 2023 Conference on Empirical Methods in Natural Language Processing (<b>EMNLP Findings 2023</b>)
</li>
</div>

<div>
<li><b>YATO: Yet Another deep learning based Text analysis Open toolkit.</b><br/>
  &nbsp;&nbsp;&nbsp;&nbsp;Zeqiang Wang*, <b>Yile Wang*</b>, Jiageng Wu, Zhiyang Teng and Jie Yang. <br/>
  &nbsp;&nbsp;&nbsp;&nbsp;In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (<b>EMNLP Demonstrations 2023</b>)
</li>
</div>

<div>
<li><b>Gradual Syntactic Label Replacement for Language Model Pre-training.</b><br/>
  &nbsp;&nbsp;&nbsp;&nbsp;<b>Yile Wang</b>, Yue Zhang, Peng Li and Yang Liu. <br/>
  &nbsp;&nbsp;&nbsp;&nbsp;In IEEE/ACM Transactions on Audio, Speech, and Language Processing (<b>TASLP 2023</b>)
</li>
</div>

<div>
  <li><b>CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition With Variational Alignment.</b><br/>
    &nbsp;&nbsp;&nbsp;&nbsp;Jiangbin Zheng, <b>Yile Wang</b>, Cheng Tan, Siyuan Li, Ge Wang, Jun Xia, Yidong Chen and Stan Z. Li. <br/>
    &nbsp;&nbsp;&nbsp;&nbsp;In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2023</b>)
  </li>
</div>

<div>
<li><b>Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings.</b><br/>
  &nbsp;&nbsp;&nbsp;&nbsp;Jiangbin Zheng*, <b>Yile Wang*</b>, Ge Wang, Jun Xia, Yufei Huang, Guojiang Zhao, Yue Zhang, Stan Z. Li. <br/>
  &nbsp;&nbsp;&nbsp;&nbsp;In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (<b>ACL 2022</b>)
</li>
</div>

<div>
  <li><b>Pre-Training a Graph Recurrent Network for Language Representation.</b><br/>
    &nbsp;&nbsp;&nbsp;&nbsp;<b>Yile Wang</b>, Linyi Yang, Zhiyang Teng, Ming Zhou and Yue Zhang. <br/>
    &nbsp;&nbsp;&nbsp;&nbsp;In the second version of the Efficient Natural Language and Speech Processing (ENLSP-II) NeurIPS workshop (<b>NeurIPS Workshop 2022</b>)
  </li>
</div>

<div>
  <li><b>Improving Skip-Gram Embeddings Using BERT.</b><br/>
    &nbsp;&nbsp;&nbsp;&nbsp;<b>Yile Wang</b>, Leyang Cui and Yue Zhang. <br/>
    &nbsp;&nbsp;&nbsp;&nbsp;In IEEE/ACM Transactions on Audio, Speech, and Language Processing (<b>TASLP 2021</b>)
  </li>
</div>

<h2 >Grants</h2>

Gradual Syntactic Label Replacement for Language Model Pre-training

